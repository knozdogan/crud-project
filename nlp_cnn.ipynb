{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "kernel75989dcf68.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/knozdogan/crud-project/blob/master/nlp_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-MGsBhR_kv8",
        "colab_type": "text"
      },
      "source": [
        "**Text Classification with CNN**\n",
        "****\n",
        "Disaster tweets classification (real or not) with convolutional neural networks.[](http://)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8FpVaicB_kwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install tensorflow-datasets\n",
        "# !pip install tf-nightly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "KenzQF_l_kwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "szDmfpVM_kwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
        "test_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
        "train_data[\"text\"][57]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23_Eh7z0_kwW",
        "colab_type": "text"
      },
      "source": [
        "*Data Preprocess*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AU24s5xc_kwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# replace urls with http\n",
        "train_data[\"text\"] = train_data[\"text\"].str.replace(\"(\\w+:\\/\\/\\S+)\", \"http\", regex=True)\n",
        "test_data[\"text\"] = test_data[\"text\"].str.replace(\"(\\w+:\\/\\/\\S+)\", \"http\", regex=True)\n",
        "\n",
        "# remove non-ascii characters\n",
        "def remove_non_ascii(text):\n",
        "    return ''.join(i for i in text if ord(i)<128)\n",
        " \n",
        "train_data['text'] = train_data['text'].apply(remove_non_ascii)\n",
        "test_data['text'] = test_data['text'].apply(remove_non_ascii)\n",
        "\n",
        "# combine two information\n",
        "train_data[\"text\"] = train_data[\"text\"] + \" in \" + train_data[\"location\"].replace(np.nan, '', regex=True)\n",
        "test_data[\"text\"] = test_data[\"text\"] + \" in \" + test_data[\"location\"].replace(np.nan, '', regex=True)\n",
        "\n",
        "# lowercase\n",
        "train_data[\"text\"] = train_data[\"text\"].str.lower()\n",
        "test_data[\"text\"] = test_data[\"text\"].str.lower()\n",
        "\n",
        "# remove punctuations\n",
        "train_data[\"text\"] = train_data[\"text\"].str.replace('[^\\w\\s]','')\n",
        "test_data[\"text\"] = test_data[\"text\"].str.replace('[^\\w\\s]','')\n",
        "\n",
        "# remove underscores\n",
        "train_data[\"text\"] = train_data[\"text\"].str.replace('_','')\n",
        "test_data[\"text\"] = test_data[\"text\"].str.replace('_','')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk2_EGiV_kwd",
        "colab_type": "text"
      },
      "source": [
        "*Input Pipeline*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GvHF41IT_kwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IYsEeeLq_kwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "x_train = train_data.pop(\"text\")\n",
        "y_train = train_data.pop(\"target\")\n",
        "dataset_train = tf.data.Dataset.from_tensor_slices((x_train.values, y_train.values))\n",
        "\n",
        "tokenizer = tfds.features.text.Tokenizer()\n",
        "vocabulary_set = set()\n",
        "for text_tensor, _ in dataset_train:\n",
        "    vocabulary_set.update(tokenizer.tokenize(text_tensor.numpy()))\n",
        "\n",
        "vocab_size = len(vocabulary_set)\n",
        "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)\n",
        "\n",
        "def encode(text_tensor, label):\n",
        "    encoded_text = encoder.encode(text_tensor.numpy())\n",
        "    return encoded_text, label\n",
        "\n",
        "def encode_map_fn(text, label):\n",
        "    # py_func doesn't set the shape of the returned tensors.\n",
        "    encoded_text, label = tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))\n",
        "    # `tf.data.Datasets` work best if all components have a shape set so set the shapes manually: \n",
        "    encoded_text.set_shape([None])\n",
        "    label.set_shape([])\n",
        "    return encoded_text, label\n",
        "\n",
        "train_data = dataset_train.map(encode_map_fn)\n",
        "\n",
        "train_data = train_data.padded_batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}